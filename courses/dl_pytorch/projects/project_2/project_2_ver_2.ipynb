{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.utils.data.sampler import WeightedRandomSampler\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torchvision import transforms\nfrom torchvision.transforms import functional as TVF\nfrom torchvision import models\n\nfrom dataclasses import dataclass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Class \nDefined Data class members"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data contants\n@dataclass\nclass DataConfiguration:\n    data_root: str = '/kaggle/input/pytorch-opencv-course-classification/'\n    images_data: str = 'images/images'\n    output_data: str = '/kaggle/working'\n    train_file_name: str = 'train.csv'\n    test_file_name: str = 'test.csv'    \n    submission_file_name: str = 'submission.csv'\n    split_ratio:int = 0.8\n    model_dir:str = 'models'\n    model_file_name:str = 'kenya_food_classifier.pt'\n    tensorboard_log_dir = 'tensorboard_kenya_food_classifier'\n    k_folds = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_config = DataConfiguration()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_sampler = False\nweighted_entropy_loss = False\nresnext50_32x4d = False\nresnext101_32x8d = True\nis_train_layer_4 = True\nuse_k_fold = True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tensorboard_log_path(parent_dir, tensorboard_log_dir):\n    log_path = os.path.join(parent_dir, tensorboard_log_dir)\n    if not os.path.exists(log_path):\n        os.chdir(parent_dir)\n        os.makedirs(tensorboard_log_dir)\n        \n    return log_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#%load_ext tensorboard\n# %reload_ext tensorboard\n\n#%tensorboard --logdir = '/kaggle/working/tensor_logs'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 0 General Functions\n\nThe following section contains general functions"},{"metadata":{},"cell_type":"markdown","source":"## 0.1 Device Type\n\nGet the Device running type"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check whether the running system is GPU/CPU\ndef GetDeviecType():\n    if torch.cuda.is_available():\n        device = \"GPU\"\n    else:\n        device = \"CPU\"\n    return device","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_device = GetDeviecType()\nprint (f'The running device is {target_device}.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0.2 Get the Labels\n\nGet the Labels of the classifier from the training file"},{"metadata":{"trusted":true},"cell_type":"code","source":"def GetLabels(data_root, train_file_name):\n    # Get the training file including its path\n    csv_path = os.path.join(data_root, train_file_name)\n    \n    #Read the CVS file using Panda\n    csv_df = pd.read_csv(csv_path, delimiter =' *, *', engine ='python')\n    #print(f'There are {csv_df.shape[0]} rows and {csv_df.shape[1]} columns in this data frame')\n    \n    #Get the Unique class type which are Labels/classes\n    classes = csv_df['class'].unique().tolist()\n    \n    return classes                                         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels =  GetLabels(data_config.data_root, data_config.train_file_name)\nprint(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class KenyanFood13Data():\n    \"\"\"\n    This custom dataset class take root directory.\n    Returns train and validation data loader.\n    \"\"\"\n            \n    def __init__(self, data_root, train_file_name, images_data):\n        \n        # Get the train csv file, including the path\n        csv_path = os.path.join(data_root, train_file_name)  \n        #Read the Train data to Data Frame\n        csv_df = pd.read_csv(csv_path, delimiter =' *, *', engine ='python')\n                \n        # initialize the data dictionary\n        self.data_dict = {\n            'image_path': [],\n            'label': []\n        }                    \n        \n        self.total_rows = csv_df.shape[0]\n        self.labels = csv_df['class'].unique().tolist()\n        \n        #Get the images path \n        images_path = os.path.join(data_root, images_data)        \n        \n        #For each entry in the csv file, prepare the image file and the corresponding clas label\n        for index in range(self.total_rows):            \n            img_file_name = os.path.join(images_path, str(csv_df['id'][index]))\n            img_file_name += '.jpg'            \n            label_index = labels.index(csv_df['class'][index])            \n            self.data_dict['image_path'].append(img_file_name)\n            self.data_dict['label'].append(label_index)         \n      \n    def length(self):\n        \"\"\"\n        return length of the data.\n        \"\"\"\n        return self.total_rows\n        \n    def get_item(self, idx):\n        \"\"\"\n        For given index, return image the class label.\n        \"\"\"\n        \n        image = self.data_dict['image_path'][idx]\n        label = self.data_dict['label'][idx]\n        \n        return image, label        \n    \n    def get_labels(self):\n        return labels        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"whole_data = KenyanFood13Data(data_config.data_root,\n                              data_config.train_file_name,\n                              data_config.images_data)\ndata_length = whole_data.length()\nlabels = whole_data.get_labels()\n\n\nprint('Data Length = ', data_length)\nprint('Data Labels = ', labels)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0.3 K-Fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"def partitions(indices, folds):\n    n_partitions = np.ones(folds) * int(len(indices)  / folds)\n    n_partitions[0 : (len(indices) % folds)] += 1\n    \n    return n_partitions\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_indices(indices, folds):\n    p = partitions(indices, folds)\n    print(p)\n    current = 0\n    \n    for index in p:\n        start = current\n        stop = current + index\n        current = stop\n        \n        yield(indices[int(start):int(stop)])\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def k_folds(indices, folds = 5):\n    '''\n        indices : No of items which needs to be folds or splits\n        folds: Number of folds\n    '''\n    fold = 0\n    for test_idx in get_indices(indices, folds):\n        train_idx = np.setdiff1d(indices, test_idx)        \n        fold = fold + 1\n        yield fold, train_idx, test_idx\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = np.arange(data_length).astype(int)\nfolds = 5\n\nfor fold_idx, train_idx, test_idx in k_folds(indices, folds):\n    print('Fold Idx = ', fold_idx)    \n    print('Train Idx =', train_idx)\n    print('Test Idx =', test_idx)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class KenyanFood13DatasetFold(Dataset):\n    \"\"\"\n    This custom dataset class take root directory.\n    Returns train and validation data loader.\n    \"\"\"\n            \n    def __init__(self, data_indices, transform = None):\n        \n        # initialize the data dictionary\n        self.data_dict = {\n            'image_path': [],\n            'label': []\n        }\n        \n        # set transform attribute\n        self.transform = transform \n        \n        #For each entry in the csv file, prepare the image file and the corresponding clas label\n        for index in range(len(data_indices)): \n            image, label = whole_data.get_item(index)\n            self.data_dict['image_path'].append(image)\n            self.data_dict['label'].append(label)         \n      \n    def __len__(self):\n        \"\"\"\n        return length of the dataset.\n        \"\"\"\n        return len(self.data_dict['label'])\n        \n    def __getitem__(self, idx):\n        \"\"\"\n        For given index, return images with resize and preprocessing and the class label.\n        \"\"\"\n        \n        image = Image.open(self.data_dict['image_path'][idx]).convert(\"RGB\")                 \n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        target = self.data_dict['label'][idx]\n\n        return image, target      \n   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Loader\nData is divided into into train and validation data, 80:20 ratio for train and validation, respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"class KenyanFood13Dataset(Dataset):\n    \"\"\"\n    This custom dataset class take root directory.\n    Returns train and validation data loader.\n    \"\"\"\n            \n    def __init__(self, data_root, train_file_name, images_data, transform, train = True, split_ratio = 0.8):\n        \n        # Get the train csv file, including the path\n        csv_path = os.path.join(data_root, train_file_name)  \n        #Read the Train data to Data Frame\n        csv_df = pd.read_csv(csv_path, delimiter =' *, *', engine ='python')\n                \n        # initialize the data dictionary\n        self.data_dict = {\n            'image_path': [],\n            'label': []\n        }\n        \n        # set transform attribute\n        self.transform = transform        \n        self.target_transform = transform\n        \n                \n        #split the data ratio into training and validation\n        total_rows = csv_df.shape[0]\n        start = 0\n        end = 0\n        \n        if (train):\n            start = int(total_rows * (1 - split_ratio))\n            end = total_rows\n        else:\n            start = 0\n            end = int(total_rows * (1 - split_ratio))\n        \n        #Get the images path \n        images_path = os.path.join(data_root, images_data)        \n        \n        #For each entry in the csv file, prepare the image file and the corresponding clas label\n        for index in range(start, end):            \n            img_file_name = os.path.join(images_path, str(csv_df['id'][index]))\n            img_file_name += '.jpg'            \n            label_index = labels.index(csv_df['class'][index])            \n            self.data_dict['image_path'].append(img_file_name)\n            self.data_dict['label'].append(label_index)         \n      \n    def __len__(self):\n        \"\"\"\n        return length of the dataset.\n        \"\"\"\n        return len(self.data_dict['label'])\n        \n    def __getitem__(self, idx):\n        \"\"\"\n        For given index, return images with resize and preprocessing and the class label.\n        \"\"\"\n        \n        image = Image.open(self.data_dict['image_path'][idx]).convert(\"RGB\")                 \n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        target = self.data_dict['label'][idx]\n\n        return image, target  \n    \n    def GetClasses(self):\n        return self.data_dict['label']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://pytorch.org/hub/pytorch_vision_resnext/ to get mean and std."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = [0.485, 0.456, 0.406] \nstd = [0.229, 0.224, 0.225]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate_test_transforms():    \n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n        ])\n    \n    return transform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_transforms():\n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),        \n        transforms.RandomRotation(20),\n        transforms.ColorJitter(0.3, 0.3, 0.3),        \n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)        \n        ])  \n    \n    return transform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_transforms():    \n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor()        \n        ])\n    \n    return transform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validate_test_transform = validate_test_transforms()\ntrain_transform = train_transforms()\ndisplay_transform = display_transforms()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the Test Loader, testing purpose\ndef test_train_loader():            \n    train_dataset =  KenyanFood13Dataset(data_config.data_root,\n                                        data_config.train_file_name,\n                                        data_config.images_data,                                        \n                                        transform = display_transform)    \n    \n    print(\"Train Data Set Length = \", train_dataset.__len__())\n    # dataloader with train dataset\n    train_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size = 8,\n            shuffle = True,\n            num_workers = 0\n        )\n\n    \n    # Plot few images\n    plt.rcParams[\"figure.figsize\"] = (15, 9)\n    plt.figure     \n    \n    #for images, targets in train_loader:\n    for batch_idx, (images, targets) in enumerate(train_loader):        \n        for i in range(len(targets)):\n            plt.subplot(3, 5, i+1)\n            img = TVF.to_pil_image(images[i])\n            plt.imshow(img)\n            plt.gca().set_title('Target: {0} : {1}'.format(targets[i],labels[targets[i]]))\n        plt.show()\n        break\n        \n#test_train_loader()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of validation samples are **1307**."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the Train Weights\ndef get_weights(train = True, debug = False):    \n    data_set = KenyanFood13Dataset(data_config.data_root,\n                                   data_config.train_file_name,\n                                   data_config.images_data,\n                                   transform = display_transform,\n                                   train = train)\n    if (debug == True):\n        if (train == True):\n            print(\"Test Data Set \")\n        else:\n            print(\"Validation Data Set \")\n            \n    \n    targets = data_set.GetClasses()\n    targets_collections = collections.Counter(targets)\n    \n    if (debug == True):\n        print('Number of Labels = {}'.format(len(targets)))\n        print(\"No. classes = \", np.unique(targets))    \n        print(\"Classes Frequency = \", targets_collections)\n    \n    target_weights = []\n    \n    for index in range(len(targets_collections)):\n        target_weights.append(targets_collections[index])    \n    \n    target_weights = [index / max(target_weights) for index in target_weights]    \n    torch_target_weights = torch.tensor(target_weights)      \n        \n    if (debug == True):\n        print(target_weights)\n        print('Maximum value = ', max(target_weights))\n        print('Normalized Weights',torch_target_weights)\n    \n    return torch_target_weights\t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_weights = get_weights(train = True, debug = True)\nvalidation_weights = get_weights(train = False, debug = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sampler_weights(train = True, debug = False):\n    \n    data_set = KenyanFood13Dataset(data_config.data_root,\n                                   data_config.train_file_name,\n                                   data_config.images_data,\n                                   transform = display_transform,\n                                   train = train)\n    if (debug == True):\n        if (train == True):\n            print(\"Test Data Set \")\n        else:\n            print(\"Validation Data Set \")\n            \n    \n    labels = data_set.GetClasses()\n    _, counts = np.unique(labels, return_counts = True)\n    \n    if (debug == True):\n        print('Number of Labels = {}'.format(len(labels)))\n        print(\"No. classes = \", np.unique(labels))    \n        print(\"Classes Frequency = \", counts)\n        \n    \n    weights = 1.0 / torch.tensor(counts, dtype=torch.float)\n    sample_weights = weights[labels]\n    sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement = True)\n    return sampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_sampler = False\ntrain_sampler = sampler_weights(train = True, debug = True)\nvalidation_sampler = sampler_weights(train = False, debug = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing the ratio of Train and validation Data Set\ntrain_dataset = KenyanFood13Dataset(data_config.data_root,\n                                        data_config.train_file_name,\n                                        data_config.images_data,                                        \n                                        transform = display_transform,\n                                        train = True)\nprint('Length of Train Data Set = ', len(train_dataset))\n\nvalidation_dataset = KenyanFood13Dataset(data_config.data_root,\n                                        data_config.train_file_name,\n                                        data_config.images_data,                                        \n                                        transform = display_transform,\n                                        train = False)\nprint('Length of Train Data Set = ', len(validation_dataset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_loader(data_root, \n                train_file_name,\n                images_data,\n                transform,\n                train,                \n                batch_size = 16,\n                shuffle = False,\n                num_workers = 2):\n    \n    dataset = KenyanFood13Dataset(data_root, train_file_name, images_data, transform, train = train)\n    \n    if (weighted_sampler == True):\n        print(\"Weighted Sampler\")\n        sampler = sampler_weights(train, debug = False)\n        loader = torch.utils.data.DataLoader(dataset,\n                                             batch_size = batch_size,\n                                             num_workers = num_workers,\n                                             shuffle = False,\n                                             sampler = sampler)\n    else:\n        loader = torch.utils.data.DataLoader(dataset,\n                                             batch_size = batch_size,\n                                             num_workers = num_workers,\n                                             shuffle = shuffle) \n                                             \n    \n    return loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  def get_data(data_root, train_file_name, images_data, batch_size, num_workers = 4, data_augmentation = False):\n    \n    # train dataloader    \n    train_loader = data_loader(data_root,\n                               train_file_name,\n                               images_data,                               \n                               transform = train_transform, \n                               train = True,\n                               batch_size = batch_size, \n                               shuffle = True, \n                               num_workers = num_workers)\n    \n    # validation dataloader    \n    validation_loader = data_loader(data_root,\n                              train_file_name,\n                              images_data,                              \n                              transform = validate_test_transform, \n                              train = False,\n                              batch_size = batch_size, \n                              shuffle = False, \n                              num_workers = num_workers)\n    \n    return train_loader, validation_loader\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_loader_fold(data_indices,                 \n                transform,                \n                batch_size = 16,\n                shuffle = False,\n                num_workers = 2):    \n    \n    data_fold = KenyanFood13DatasetFold(data_indices, transform = transform)\n    loader = torch.utils.data.DataLoader(data_fold,\n                                             batch_size = batch_size,\n                                             num_workers = num_workers,\n                                             shuffle = shuffle) \n                                             \n    \n    return loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data_fold(train_idx, test_idx, batch_size, num_workers = 4, data_augmentation = False):\n    \n    # train dataloader    \n    train_loader = data_loader_fold(train_idx,                                                              \n                               transform = train_transform,                                \n                               batch_size = batch_size, \n                               shuffle = True, \n                               num_workers = num_workers)\n    \n    # validation dataloader    \n    validation_loader = data_loader_fold(test_idx,                              \n                              transform = validate_test_transform,                               \n                              batch_size = batch_size, \n                              shuffle = False, \n                              num_workers = num_workers)\n    \n    return train_loader, validation_loader\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. System Configuration\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"@dataclass\nclass TrainingConfiguration: \n    ''' \n    Describes configuration of the training process\n    ''' \n    batch_size: int = 8 \n    epochs_count: int = 8\n    init_learning_rate: float = 0.0001 # initial learning rate for lr scheduler \n    decay_rate:float = 0.1\n    log_interval: int = 5\n    test_interval: int = 1\n    num_workers: int = 4 \n    device: str = 'cuda'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@dataclass\nclass SystemConfiguration:\n    '''\n    Describes the common system setting needed for reproducible training\n    '''\n    seed: int = 21  # seed number to set the state of all random number generators\n    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def setup_system(system_config: SystemConfiguration) -> None:\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(\n    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n    train_loader: torch.utils.data.DataLoader, epoch_idx: int, tb_writer: SummaryWriter\n) -> None:\n    \n    # change model in training mood\n    model.train()\n    \n    # to get batch loss\n    batch_loss = np.array([])\n    \n    # to get batch accuracy\n    batch_acc = np.array([])\n     \n    if (weighted_entropy_loss):\n        train_weights = get_weights(True)    \n        train_weights = train_weights.to(train_config.device)\n    \n    for batch_idx, (data, target) in enumerate(train_loader):\n        \n        # clone target\n        indx_target = target.clone()\n        # send data to device (its is medatory if GPU has to be used)\n        data = data.to(train_config.device)\n        # send target to device\n        target = target.to(train_config.device)\n\n        # reset parameters gradient to zero\n        optimizer.zero_grad()\n        \n        # forward pass to the model\n        output = model(data)\n        \n        # cross entropy loss\n        \n        if (weighted_entropy_loss):\n            criterion = nn.CrossEntropyLoss(weight = train_weights)\n            loss = criterion(output, target)\n        else:\n            loss = F.cross_entropy(output, target)\n        \n        # find gradients w.r.t training parameters\n        loss.backward()\n        # Update parameters using gardients\n        optimizer.step()\n        \n        batch_loss = np.append(batch_loss, [loss.item()])\n        \n        # Score to probability using softmax\n        prob = F.softmax(output, dim=1)\n            \n        # get the index of the max probability\n        pred = prob.data.max(dim=1)[1]  \n                        \n        # correct prediction\n        correct = pred.cpu().eq(indx_target).sum()\n            \n        # accuracy\n        acc = float(correct) / float(len(data))\n        \n        batch_acc = np.append(batch_acc, [acc])\n        \n        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:            \n            total_batch = epoch_idx * len(train_loader.dataset) / train_config.batch_size + batch_idx\n            tb_writer.add_scalar('Loss/train-batch', loss.item(), total_batch)\n            tb_writer.add_scalar('Accuracy/train-batch', acc, total_batch)\n            \n    epoch_loss = batch_loss.mean()\n    epoch_acc = batch_acc.mean()\n    print('Epoch: {} \\nTrain Loss: {:.6f} Acc: {:.4f}'.format(epoch_idx, epoch_loss, epoch_acc))\n    \n    return epoch_loss, epoch_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate(\n    train_config: TrainingConfiguration,\n    model: nn.Module,\n    test_loader: torch.utils.data.DataLoader,\n) -> float:\n    # \n    model.eval()\n    test_loss = 0\n    count_corect_predictions = 0\n    \n    if (weighted_entropy_loss):\n        validation_weights = get_weights(False)\n        validation_weights = validation_weights.to(train_config.device)\n        \n    for data, target in test_loader:\n        indx_target = target.clone()\n        data = data.to(train_config.device)\n        \n        target = target.to(train_config.device)\n        \n        output = model(data)\n        # add loss for each mini batch\n        \n        if (weighted_entropy_loss):        \n            criterion = nn.CrossEntropyLoss(weight = validation_weights)\n            test_loss += criterion(output, target).item()\n        else:\n            test_loss += F.cross_entropy(output, target).item()\n        \n        # Score to probability using softmax\n        prob = F.softmax(output, dim=1)\n        \n        # get the index of the max probability\n        pred = prob.data.max(dim=1)[1] \n        \n        # add correct prediction count\n        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n\n    # average over number of mini-batches\n    test_loss = test_loss / len(test_loader)  \n    \n    # average over number of dataset\n    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n    \n    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.\n          format(test_loss, count_corect_predictions, len(test_loader.dataset), accuracy))\n    \n    return test_loss, accuracy/100.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(model, device, parent_dir, model_dir = 'models', model_file_name ='kenya_food_classifier.pt'):\n    \n    data_path = os.path.join(parent_dir, model_dir) \n    if not os.path.exists(data_path):\n        os.chdir(parent_dir)\n        os.makedirs(model_dir)\n\n    model_path = os.path.join(model_dir, model_file_name)\n    #print(\"Model Path =\", model_path)\n\n    # make sure you transfer the model to cpu.\n    if device == 'cuda':\n        model.to('cpu')\n\n    # save the state_dict\n    torch.save(model.state_dict(), model_path)\n    \n    if device == 'cuda':\n        model.to('cuda')\n    \n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_model(model, parent_dir, model_dir = 'models', model_file_name = 'kenya_food_classifier.pt'):\n    model_path = os.path.join(parent_dir, model_dir, model_file_name)\n    print('Model Path =', model_path)\n    # loading the model and getting model parameters by using load_state_dict\n    model.load_state_dict(torch.load(model_path))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def main(model, optimizer, tb_writer, scheduler = None, system_configuration = SystemConfiguration(), \n         training_configuration = TrainingConfiguration(), \n         data_configuration = DataConfiguration(), data_augmentation = True):\n    \n    # system configuration\n    setup_system(system_configuration)\n\n    # batch size\n    batch_size_to_set = training_configuration.batch_size\n    # num_workers\n    num_workers_to_set = training_configuration.num_workers\n    # epochs\n    epoch_num_to_set = training_configuration.epochs_count\n\n    # if GPU is available use training config, \n    # else lowers batch_size, num_workers and epochs count\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n        batch_size_to_set = 16\n        num_workers_to_set = 4\n\n    # data loader\n    train_loader, test_loader = get_data(\n        batch_size = batch_size_to_set,\n        data_root = data_configuration.data_root,\n        train_file_name = data_configuration.train_file_name,\n        images_data = data_configuration.images_data,\n        num_workers = num_workers_to_set,\n        data_augmentation = data_augmentation\n    )\n    \n    # Update training configuration\n    training_configuration = TrainingConfiguration(\n        device=device,\n        batch_size=batch_size_to_set,\n        num_workers=num_workers_to_set\n    )\n        \n    # send model to device (GPU/CPU)\n    model.to(training_configuration.device)\n\n    best_loss = torch.tensor(np.inf)\n    \n    # epoch train/test loss\n    epoch_train_loss = np.array([])\n    epoch_test_loss = np.array([])\n    \n    # epch train/test accuracy\n    epoch_train_acc = np.array([])\n    epoch_test_acc = np.array([])\n    \n    # trainig time measurement\n    t_begin = time.time()\n    for epoch in range(training_configuration.epochs_count):\n        \n#         Calculate Initial Test Loss\n        #init_val_loss, init_val_accuracy = validate(training_configuration, model, test_loader)\n        #print(\"Initial Test Loss : {:.6f}, Initial Test Accuracy : {:.3f}%\".format(init_val_loss, init_val_accuracy*100))\n        \n        # Train\n        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch, tb_writer)\n        \n        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n        \n        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n        \n        # add scalar (loss/accuracy) to tensorboard\n        tb_writer.add_scalar('Loss/Train',train_loss, epoch)\n        tb_writer.add_scalar('Accuracy/Train', train_acc, epoch)\n\n        elapsed_time = time.time() - t_begin\n        speed_epoch = elapsed_time / (epoch + 1)\n        speed_batch = speed_epoch / len(train_loader)\n        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n        \n        print(\"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n                elapsed_time, speed_epoch, speed_batch, eta))\n        \n        # add time metadata to tensorboard\n        tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch)\n        tb_writer.add_scalar('Time/speed_epoch', speed_epoch, epoch)\n        tb_writer.add_scalar('Time/speed_batch', speed_batch, epoch)\n        tb_writer.add_scalar('Time/eta', eta, epoch)\n\n        # Validate\n        if epoch % training_configuration.test_interval == 0:\n            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n            \n            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n        \n            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n            \n            # add scalar (loss/accuracy) to tensorboard\n            tb_writer.add_scalar('Loss/Validation', current_loss, epoch)\n            tb_writer.add_scalar('Accuracy/Validation', current_accuracy, epoch)\n            \n            # add scalars (loss/accuracy) to tensorboard\n            tb_writer.add_scalars('Loss/train-val', {'train': train_loss, \n                                           'validation': current_loss}, epoch)\n            tb_writer.add_scalars('Accuracy/train-val', {'train': train_acc, \n                                               'validation': current_accuracy}, epoch)\n            \n            if current_loss < best_loss:\n                best_loss = current_loss                \n                save_model(model, device = training_configuration.device, \n                           parent_dir = data_configuration.output_data)\n                print('Model Improved. Saved the Model...')\n        \n                \n    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n    \n    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def main_fold(model, optimizer, tb_writer, scheduler = None, system_configuration = SystemConfiguration(), \n         training_configuration = TrainingConfiguration(), \n         data_configuration = DataConfiguration(), data_augmentation = True):\n    \n    # system configuration\n    setup_system(system_configuration)\n\n    # batch size\n    batch_size_to_set = training_configuration.batch_size\n    # num_workers\n    num_workers_to_set = training_configuration.num_workers\n    # epochs\n    epoch_num_to_set = training_configuration.epochs_count\n\n    # if GPU is available use training config, \n    # else lowers batch_size, num_workers and epochs count\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n        batch_size_to_set = 16\n        num_workers_to_set = 4\n\n    # data loader\n    train_loader, test_loader = get_data(\n        batch_size = batch_size_to_set,\n        data_root = data_configuration.data_root,\n        train_file_name = data_configuration.train_file_name,\n        images_data = data_configuration.images_data,\n        num_workers = num_workers_to_set,\n        data_augmentation = data_augmentation\n    )\n    \n    # Update training configuration\n    training_configuration = TrainingConfiguration(\n        device=device,\n        batch_size=batch_size_to_set,\n        num_workers=num_workers_to_set\n    )\n        \n    # send model to device (GPU/CPU)\n    model.to(training_configuration.device)\n\n    best_loss = torch.tensor(np.inf)\n    \n    # epoch train/test loss\n    epoch_train_loss = np.array([])\n    epoch_test_loss = np.array([])\n    \n    # epch train/test accuracy\n    epoch_train_acc = np.array([])\n    epoch_test_acc = np.array([])\n    \n    # trainig time measurement\n    t_begin = time.time()\n    \n    indices = np.arange(data_length).astype(int)\n    folds = 5\n\n    for fold_idx, train_idx, test_idx in k_folds(indices, folds): \n        for epoch in range(training_configuration.epochs_count): \n            train_loader, test_loader = get_data_fold(train_idx, test_idx,\n                                                 batch_size = batch_size_to_set,\n                                                 num_workers = num_workers_to_set,\n                                                 data_augmentation = data_augmentation\n            )\n            \n\n            # Train\n            train_loss, train_acc = train(training_configuration,\n                                          model,\n                                          optimizer, train_loader, epoch, tb_writer)\n        \n            epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n        \n            epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n        \n            # add scalar (loss/accuracy) to tensorboard\n            tb_writer.add_scalar('Loss/Train',train_loss, epoch)\n            tb_writer.add_scalar('Accuracy/Train', train_acc, epoch)\n\n            elapsed_time = time.time() - t_begin\n            speed_epoch = elapsed_time / (epoch + 1)\n            speed_batch = speed_epoch / len(train_loader)\n            eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n\n            print(\"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n                    elapsed_time, speed_epoch, speed_batch, eta))\n        \n            # add time metadata to tensorboard\n            tb_writer.add_scalar('Time/elapsed_time', elapsed_time, epoch)\n            tb_writer.add_scalar('Time/speed_epoch', speed_epoch, epoch)\n            tb_writer.add_scalar('Time/speed_batch', speed_batch, epoch)\n            tb_writer.add_scalar('Time/eta', eta, epoch)\n\n            # Validate\n            if epoch % training_configuration.test_interval == 0:\n                current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n\n                epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n\n                epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n\n                # add scalar (loss/accuracy) to tensorboard\n                tb_writer.add_scalar('Loss/Validation', current_loss, epoch)\n                tb_writer.add_scalar('Accuracy/Validation', current_accuracy, epoch)\n\n                # add scalars (loss/accuracy) to tensorboard\n                tb_writer.add_scalars('Loss/train-val', {'train': train_loss, \n                                               'validation': current_loss}, epoch)\n                tb_writer.add_scalars('Accuracy/train-val', {'train': train_acc, \n                                                   'validation': current_accuracy}, epoch)\n\n                if current_loss < best_loss:\n                    best_loss = current_loss                \n                    save_model(model, device = training_configuration.device, \n                               parent_dir = data_configuration.output_data)\n                    print('Model Improved. Saved the Model...')\n        print(\"Fold completed {}.\".format(fold_idx))\n        \n                \n    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n    \n    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss_accuracy(train_loss, val_loss, train_acc, val_acc, colors, \n                       loss_legend_loc='upper center', acc_legend_loc='upper left', \n                       fig_size=(20, 10), sub_plot1=(1, 2, 1), sub_plot2=(1, 2, 2)):\n    \n    plt.rcParams[\"figure.figsize\"] = fig_size\n    fig = plt.figure()\n    \n    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n    \n    for i in range(len(train_loss)):\n        x_train = range(len(train_loss[i]))\n        x_val = range(len(val_loss[i]))\n        \n        min_train_loss = train_loss[i].min()\n        \n        min_val_loss = val_loss[i].min()\n        \n        plt.plot(x_train, train_loss[i], linestyle='-', color='tab:{}'.format(colors[i]), \n                 label=\"TRAIN LOSS ({0:.4})\".format(min_train_loss))\n        plt.plot(x_val, val_loss[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n                 label=\"VALID LOSS ({0:.4})\".format(min_val_loss))\n        \n    plt.xlabel('epoch no.')\n    plt.ylabel('loss')\n    plt.legend(loc=loss_legend_loc)\n    plt.title('Training and Validation Loss')\n        \n    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n    \n    for i in range(len(train_acc)):\n        x_train = range(len(train_acc[i]))\n        x_val = range(len(val_acc[i]))\n        \n        max_train_acc = train_acc[i].max() \n        \n        max_val_acc = val_acc[i].max() \n        \n        plt.plot(x_train, train_acc[i], linestyle='-', color='tab:{}'.format(colors[i]), \n                 label=\"TRAIN ACC ({0:.4})\".format(max_train_acc))\n        plt.plot(x_val, val_acc[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n                 label=\"VALID ACC ({0:.4})\".format(max_val_acc))\n        \n    plt.xlabel('epoch no.')\n    plt.ylabel('accuracy')\n    plt.legend(loc=acc_legend_loc)\n    plt.title('Training and Validation Accuracy')\n    \n    fig.savefig('sample_loss_acc_plot.png')\n    plt.show()\n    \n    return   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pretrained_resnext(transfer_learning = True, num_class = 13):\n    #resnet = models.resnext101_32x8d(pretrained = True)\n    \n    if (resnext50_32x4d == True):\n        resnet = models.resnext50_32x4d(pretrained = True)\n     \n    if (resnext101_32x8d == True):\n        resnet = models.resnext101_32x8d(pretrained = True)\n        \n        \n    #for name, child in resnet.named_children():\n        #print(name)\n    if (is_train_layer_4 == True):\n        for name, child in resnet.named_children():\n            if name in ['layer4']:\n                print(name + ' has been unfrozen.')\n                for param in child.parameters():\n                    param.requires_grad = True\n            else:\n                for param in child.parameters():\n                    param.requires_grad = False        \n            \n    last_layer_in = resnet.fc.in_features\n    resnet.fc = nn.Linear(last_layer_in, num_class)\n    \n    return resnet    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = pretrained_resnext(transfer_learning=True)\n#print(model)\n\n# get optimizer\ntrain_config = TrainingConfiguration()\n\n# optimizer\n\n\noptimizer = optim.SGD(\n    model.parameters(),\n    lr = train_config.init_learning_rate,\n    momentum = 0.9\n)\n\ndecay_rate = train_config.decay_rate\n\nlmbda = lambda epoch: 1/(1 + decay_rate * epoch)\n\n# Scheduler\n#scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lmbda)\nscheduler = lr_scheduler.StepLR(optimizer, step_size = 7, gamma = 0.1)\n\n\n# Tensorboard summary writer\nlog_sw = SummaryWriter('/kaggle/working/tensor_logs/resnext101_32x8d')   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train and validate\nif (use_k_fold == True):\n     model, train_loss, train_acc, val_loss, val_acc = main_fold(model,\n                                                                 optimizer,\n                                                                 log_sw,\n                                                                 scheduler = scheduler,\n                                                                 data_augmentation = True)\nelse:    \n    model, train_loss, train_acc, val_loss, val_acc = main(model, optimizer, log_sw, scheduler = scheduler, data_augmentation = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss_accuracy(train_loss=[train_loss], \n                   val_loss=[val_loss], \n                   train_acc=[train_acc], \n                   val_acc=[val_acc], \n                   colors=['blue'], \n                   loss_legend_loc='upper center', \n                   acc_legend_loc='upper left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class KenyanFood13Testset(Dataset):\n    \"\"\"\n    This custom dataset class take root directory.    \n    \"\"\"\n    \n    def __init__(self, data_root, test_file_name, images_data, transform):\n    \n        # Get the test csv file, including the path\n        csv_path = os.path.join(data_root, test_file_name)  \n        #Read the Train data to Data Frame\n        csv_df = pd.read_csv(csv_path, delimiter =' *, *', engine ='python')\n        #print(csv_df)        \n        #print(f'There are {csv_df.shape[0]} rows and {csv_df.shape[1]} columns in this data frame')\n        #csv_df.info()\n                        \n       # initialize the data dictionary\n        self.data_dict = {\n            'image_path': [],\n            'label': [] \n        }\n \n        # set transform attribute\n        self.transform = transform\n        \n        #Get the images path \n        images_path = os.path.join(data_root, images_data)        \n \n        for index in range (csv_df.shape[0]):\n            img_file_name = os.path.join(images_path, str(csv_df['id'][index]))\n            img_file_name += '.jpg'  \n            self.data_dict['image_path'].append(img_file_name)\n            self.data_dict['label'].append(0)\n            \n    def __len__(self):\n        \"\"\"\n        return length of the dataset.\n        \"\"\"\n        return len(self.data_dict['label'])\n\n    def __getitem__(self, idx):\n        \"\"\"\n        For given index, return images with resize and preprocessing and the class label.\n        \"\"\"\n\n        image = Image.open(self.data_dict['image_path'][idx]).convert(\"RGB\")        \n        image = self.transform(image)            \n        target = self.data_dict['label'][idx]\n\n        return image, target  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction(model, device, batch_input,  max_prob = True):\n    \n    # send model to cpu/cuda according to your system configuration\n    model.to(device)\n    \n    # it is important to do model.eval() before prediction\n    model.eval()\n\n    data = batch_input.to(device)\n\n    output = model(data)\n\n    # Score to probability using softmax\n    prob = F.softmax(output, dim=1)\n\n    # get the max probability\n    if max_prob:\n        # get the max probability\n        pred_prob = prob.data.max(dim=1)[0]\n    else:\n        pred_prob = prob.data\n    \n    # get the index of the max probability\n    pred_index = prob.data.max(dim=1)[1]\n    \n    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_target_and_prob(model, dataloader, device):\n    \"\"\"\n    get targets and prediction probabilities\n    \"\"\"\n    \n    pred_prob = []\n    pred_targets = []\n    \n    for _, (data, target) in enumerate(dataloader):\n        \n        pre_target, prob = prediction(model, device, data, max_prob = False)\n        #print(pre_target)\n        pred_prob.append(prob)                \n        pred_targets.append(pre_target)\n        \n    pred_targets = np.concatenate(pred_targets)    \n    pred_prob = np.concatenate(pred_prob, axis=0)\n    \n    return pred_targets, pred_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_predictions():\n    data_config = DataConfiguration()\n    preprocess =  validate_test_transform   \n    \n    if torch.cuda.is_available():\n        device = \"cuda\"\n        num_workers = 8\n    else:\n        device = \"cpu\"\n        num_workers = 2\n        \n    test_dataset =  KenyanFood13Testset(data_config.data_root, data_config.test_file_name, data_config.images_data, preprocess)\n    print(\"Test Data Set Length = \", test_dataset.__len__())\n    # dataloader with dataset\n    test_loader = torch.utils.data.DataLoader(\n            test_dataset,\n            batch_size = 16,\n            shuffle = False,\n            num_workers = 2\n        )\n    \n    model = pretrained_resnext(transfer_learning = True)\n    model = load_model(model, parent_dir = data_config.output_data)\n    pred_targets, pred_prob = get_target_and_prob(model, test_loader, device)\n    \n    return pred_targets, pred_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_targets, pred_prob = get_test_predictions()\nprint('No. of Predictions = ', len(pred_targets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_submission(data_root, test_file_name, working_root, submission_file_name, targets, labels):\n    test_path = os.path.join(data_root, test_file_name)\n    test_data_df = pd.read_csv(test_path, delimiter =' *, *', engine ='python')\n    \n    \n    ids = []\n    classes = []\n    if (test_data_df.shape[0] == len(targets)):\n        for index in range (test_data_df.shape[0]):\n            image_id = test_data_df['id'][index]\n            ids.append(image_id)\n            classes.append(labels[targets[index]])\n    \n        \n    # dictionary of lists  \n    dict = {'id': ids, 'class': classes}\n    \n    submission_data_df = pd.DataFrame(dict)\n    \n    submission_path = os.path.join(working_root, submission_file_name)\n    submission_data_df.to_csv(submission_path, index = False)    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_config = DataConfiguration()\ngenerate_submission(data_config.data_root, data_config.test_file_name, data_config.output_data, data_config.submission_file_name, pred_targets, labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%tensorboard --logdir /kaggle/working/tensor_logs/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%tensorboard dev upload --logdir /kaggle/working/tensor_logs/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kaggle link\nhttps://www.kaggle.com/premchedella/project-2"},{"metadata":{},"cell_type":"markdown","source":"# Lessons Learned\n\n* resnext50_32x4d \n    * Unfreezed Layer 4 did not give promising results\n* resnext101_32x8d\n    * Unfreezed Layer 4 did not give promising results\n    * Training all layers with taking first 20% for validation and remaining 80% for training gives an 75.4% accuracy on test set.\n* Heavy augmentation\n    * Added Randmon Vertical, Random Affine, and Random Erasing -- For 20 ephocs, results to 75.33%\n    * Removed Random Vertical, Random Affine, and Random Erasing -- For 20, epochs, results to 74.2%\n    \n* Weighted Cross Entropy\n    * Test Accuracy is 79.1% , validation accuracy is 76.8%,and test accuracy is \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}