{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">Assignment: LinkNet Architecture with VGG16</font>\n",
    "\n",
    "We have already implemented the LinkNet with backbone `ResNet18` in the course. In this assignment, you have to implement the LinkNet Architecture with backbone `VGG16`.\n",
    "\n",
    "## <font color='blue'>Marking Scheme</font>\n",
    "\n",
    "#### Maximum Points: 30\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Problem</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>3. LinkNet Implementation</h3></td> <td><h3>30</h3></td> </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:green\">1. LinkNet</font>\n",
    "\n",
    "Let's briefly overview the LinkNet architecture. [LinkNet](https://arxiv.org/pdf/1707.03718.pdf) was\n",
    "introduced in 2017 by A.Chaurasia and E.Culurciello as a novel lightweight deep neural network for semantic\n",
    "segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w11-LinkNet_architecture.png'>\n",
    "\n",
    "---\n",
    "\n",
    "In the picture above `/2` means downsampling of the feature map by a factor of 2 which is achieved by performing strided convolution, `âˆ—2` denotes upsampling by `2`.\n",
    "\n",
    "\n",
    "An encoder is the left half of the network, whereas the the right side of it is a decoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:green\">2. Implementation Guidelines </font>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w11-VGG16-architecture-16.png'>\n",
    "<center>Image credits: www.researchgate.net</center>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**1. The following block (before encoder) will change as follows for VGG16:**\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w11-assignment-1.png'>\n",
    "\n",
    "- `conv[(7x7), (3, 64), /2]` will be replaced by the first convolution layer of `VGG16` (`conv[(3x3), (3, 64)]`).\n",
    "\n",
    "\n",
    "- `max-pool[(3x3), /2]` will be replaced by the first max-pool layer of `VGG16` (`max-pool[(2x2), /2]`).\n",
    "\n",
    "\n",
    "**2. Sub-sequent convolution layers till max-pool (including max-pool) will be used as one encoder blocks. For example, fourth (`convolution + ReLU`), fifth (`convolution + ReLU`), and sixth (`max pooling`) layer combined will be used as `Encoder Block 1`.**\n",
    "\n",
    "\n",
    "**3. The decoder code is already given, you don't have to change anything for the decoder.**\n",
    "\n",
    "**4. The number of output channels of `decoder block 1` must be `64`. Although logically, it can be any number.**\n",
    "\n",
    "\n",
    "**5. The block after `decoder block 1`, let's call it `classifier`. You have to write code for the block such that it takes care of the number of classes and input image width and height (output size of the classifier = `[batch_size, num_classes, height, width]`)**\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w11-assignment2.png'>\n",
    "\n",
    "- Replace the above block with your classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:green\">3. LinkNet Implementation [30 Points]</font>\n",
    "\n",
    "\n",
    "**The LinkNet implementation points divided into the following two parts:**\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Problem</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Encoder-Decoder Implementation</h3></td> <td><h3>15</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Classifier & Forward Implementation</h3></td> <td><h3>15</h3></td> </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch neural network (NN) module for building and training nets\n",
    "import torch.nn as nn\n",
    "# module with various model definitions\n",
    "import torchvision.models as models\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## <font style=\"color:green\">3.1. Decoder</font>\n",
    "\n",
    "The below presented block is a decoder, which takes a feature map with defined channels number. The `channels_in` the result map should be equal to `channels_out`.\n",
    "\n",
    "We have used `ConvTranspose2d` for upsampling, find details [here](https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# create decoder block inherited from nn.Module\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1x1 projection module to reduce channels\n",
    "        self.proj = nn.Sequential(\n",
    "            # convolution\n",
    "            nn.Conv2d(channels_in, channels_in // 2, kernel_size=1, bias=False),\n",
    "            # batch normalization\n",
    "            nn.BatchNorm2d(channels_in // 2),\n",
    "            # relu activation\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # fully convolutional module\n",
    "        self.deconv = nn.Sequential(\n",
    "            # deconvolution\n",
    "            nn.ConvTranspose2d(\n",
    "                channels_in // 2,\n",
    "                channels_in // 2,\n",
    "                kernel_size=2,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                output_padding=0,\n",
    "                groups=channels_in // 2,\n",
    "                bias=False\n",
    "            ),\n",
    "            # batch normalization\n",
    "            nn.BatchNorm2d(channels_in // 2),\n",
    "            # relu activation\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 1x1 unprojection module to increase channels\n",
    "        self.unproj = nn.Sequential(\n",
    "            # convolution\n",
    "            nn.Conv2d(channels_in // 2, channels_out, kernel_size=1, bias=False),\n",
    "            # batch normalization\n",
    "            nn.BatchNorm2d(channels_out),\n",
    "            # relu activation\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    # stack layers and perform a forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        proj = self.proj(x)\n",
    "        deconv = self.deconv(proj)\n",
    "        unproj = self.unproj(deconv)\n",
    "\n",
    "        return unproj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## <font style=\"color:green\">3.2. LinkNet</font>\n",
    "\n",
    "**Write your code where it is specified. Do not modify / delete other codes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LinkNet model with VGG16 encoder\n",
    "class LinkNet(nn.Module):\n",
    "    def __init__(self, num_classes, encoder=\"vgg16\"):\n",
    "        super().__init__()\n",
    "        assert hasattr(models, encoder), \"Undefined encoder type\"\n",
    "        # prepare feature extractor from `torchvision` vgg16 model\n",
    "        \n",
    "        vgg16 = getattr(models, encoder)(pretrained=False)\n",
    "        \n",
    "        ###########################################################################################\n",
    "        # write code for self.init and self.maxpool\n",
    "        # for self.init: the first convolution layer of VGG16 (conv[(3x3), (3, 64)]).\n",
    "        # for self.maxpool: the first max-pool layer of VGG16 (max-pool[(2x2), /2]).\n",
    "        ###########################################################################################\n",
    "        \n",
    "        self.init = None\n",
    "        self.maxpool = None\n",
    "        \n",
    "        ###\n",
    "        ### YOUR CODE HERE\n",
    "        #print(vgg16)\n",
    "        #print(len(vgg16.features))\n",
    "       \n",
    "        # All the features of the VGG 16\n",
    "        encoder = list(models.vgg16().children())[0][:len(vgg16.features)]\n",
    "        # First convolution layer, encoder[0]\n",
    "        self.init = encoder[0:4]        \n",
    "        # First max pool layer, encoder[4]\n",
    "        self.maxpool = encoder[4]\n",
    "        ###\n",
    "        \n",
    "        ############################################################################################\n",
    "        # Write code for encoder blocks:\n",
    "        # Sub-sequent convolution layers and max-pool combined will be used as encoder blocks. \n",
    "        # Let's encoder blocks is named as self.layer1, self.layer2, self.layer3, and self.layer4 for \n",
    "        # encoder block1, encoder block2, encoder block3, and encoder block4 respectively.\n",
    "        ############################################################################################\n",
    "        \n",
    "        self.layer1 = None\n",
    "        self.layer2 = None\n",
    "        self.layer3 = None\n",
    "        self.layer4 = None\n",
    "        \n",
    "        ###\n",
    "        ### YOUR CODE HERE\n",
    "        self.layer1 = encoder[5:10]\n",
    "        self.layer2 = encoder[10:17]\n",
    "        self.layer3 = encoder[17:24]\n",
    "        self.layer4 = encoder[24:31]    \n",
    "        ###\n",
    "\n",
    "        \n",
    "        #############################################################################################\n",
    "        # Decoder's block: DecoderBlock module\n",
    "        \n",
    "        # Write code for decoder block here. As DecoderBlock class is already defined, you have to \n",
    "        # initiate the class with arguments channels_in and channels_out. \n",
    "        \n",
    "        # Let's decoder block as self.up4, self.up3, self.up2, self.up1 for decoder block4, \n",
    "        # decoder block3, decoder block2, and decoder block1 respectively. \n",
    "        #############################################################################################\n",
    "        \n",
    "        self.up4 = None\n",
    "        self.up3 = None\n",
    "        self.up2 = None\n",
    "        \n",
    "        # output channel of self.up1 must be 64\n",
    "        self.up1 = None\n",
    "\n",
    "        ###\n",
    "        ### YOUR CODE HERE        \n",
    "        \n",
    "        self.up4 = DecoderBlock(channels_in=512, channels_out=512)\n",
    "        self.up3 = DecoderBlock(channels_in=512, channels_out=256)\n",
    "        self.up2 = DecoderBlock(channels_in=256, channels_out=128)\n",
    "        self.up1 = DecoderBlock(channels_in=128, channels_out=64)\n",
    "        ###\n",
    "\n",
    "        # Classification block: define a classifier module\n",
    "        \n",
    "        ################################################################################################\n",
    "        # You have to write the classifier part as a Sequential model.\n",
    "        ################################################################################################\n",
    "        self.classifier = nn.Sequential(\n",
    "            ###\n",
    "            ### YOUR CODE HERE\n",
    "            # deconvolution layer\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, bias=False),\n",
    "            # batch normalization with num_features = 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            # activation function\n",
    "            nn.ReLU(),\n",
    "            # convolutional layer\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=False),\n",
    "            # batch normalization with num_features = 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            # activation function\n",
    "            nn.ReLU(),\n",
    "            # convolutional layer\n",
    "            nn.Conv2d(32, num_classes, kernel_size=2, padding=0)          \n",
    "            ###\n",
    "        )        \n",
    "        \n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #############################################################################################\n",
    "        # You have to complete the forward method for LinkNet.\n",
    "        #############################################################################################\n",
    "        \n",
    "        # for input image size (3, 320, 320)\n",
    "\n",
    "        # output size = (64, 320, 320)\n",
    "        init = self.init(x)\n",
    "        # output size = (64, 160, 160)\n",
    "        maxpool = self.maxpool(init)\n",
    "        \n",
    "        ###\n",
    "        ### YOUR CODE HERE        \n",
    "        layer1 = self.layer1(maxpool)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "        up4 = self.up4(layer4) + layer3\n",
    "        up3 = self.up3(up4) + layer2\n",
    "        up2 = self.up2(up3) + layer1\n",
    "        up1 = self.up1(up2)\n",
    "        ###\n",
    "\n",
    "        # output size = (5, 320, 320), where 5 is the predefined number of classes\n",
    "        output = self.classifier(up1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# LinkNet architecture\n",
    "#model = LinkNet(num_classes=5, encoder=\"vgg16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:green\">4. Check the implementation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## <font style=\"color:green\">4.1. Check Encoder-Decoder Implementation with Model Profiler</font>\n",
    "Verify your encoder-decoder implementation with the model profiler before submitting it. \n",
    "Here, we will check the number of floating points operation and the number of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class ModelProfiler(nn.Module):\n",
    "    \"\"\" Profile PyTorch models.\n",
    "\n",
    "    Compute FLOPs (FLoating OPerations) and number of trainable parameters of model.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): model which will be profiled.\n",
    "\n",
    "    Example:\n",
    "        model = torchvision.models.resnet50()\n",
    "        profiler = ModelProfiler(model)\n",
    "        var = torch.zeros(1, 3, 224, 224)\n",
    "        profiler(var)\n",
    "        print(\"FLOPs: {0:.5}; #Params: {1:.5}\".format(profiler.get_flops('G'), profiler.get_params('M')))\n",
    "\n",
    "    Warning:\n",
    "        Model profiler doesn't work with models, wrapped by torch.nn.DataParallel.\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.flops = 0\n",
    "        self.units = {'K': 10.**3, 'M': 10.**6, 'G': 10.**9}\n",
    "        self.hooks = None\n",
    "        self._remove_hooks()\n",
    "\n",
    "    def get_flops(self, units='G'):\n",
    "        \"\"\" Get number of floating operations per inference.\n",
    "\n",
    "        Arguments:\n",
    "            units (string): units of the flops value ('K': Kilo (10^3), 'M': Mega (10^6), 'G': Giga (10^9)).\n",
    "\n",
    "        Returns:\n",
    "            Floating operations per inference at the choised units.\n",
    "        \"\"\"\n",
    "        assert units in self.units\n",
    "        return self.flops / self.units[units]\n",
    "\n",
    "    def get_params(self, units='G'):\n",
    "        \"\"\" Get number of trainable parameters of the model.\n",
    "\n",
    "        Arguments:\n",
    "            units (string): units of the flops value ('K': Kilo (10^3), 'M': Mega (10^6), 'G': Giga (10^9)).\n",
    "\n",
    "        Returns:\n",
    "            Number of trainable parameters of the model at the choised units.\n",
    "        \"\"\"\n",
    "        assert units in self.units\n",
    "        params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        if units is not None:\n",
    "            params = params / self.units[units]\n",
    "        return params\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        self.flops = 0\n",
    "        self._init_hooks()\n",
    "        output = self.model(*args, **kwargs)\n",
    "        self._remove_hooks()\n",
    "        return output\n",
    "\n",
    "    def _remove_hooks(self):\n",
    "        if self.hooks is not None:\n",
    "            for hook in self.hooks:\n",
    "                hook.remove()\n",
    "        self.hooks = None\n",
    "\n",
    "    def _init_hooks(self):\n",
    "        self.hooks = []\n",
    "\n",
    "        def hook_compute_flop(module, _, output):\n",
    "            self.flops += module.weight.size()[1:].numel() * output.size()[1:].numel()\n",
    "\n",
    "        def add_hooks(module):\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                self.hooks.append(module.register_forward_hook(hook_compute_flop))\n",
    "\n",
    "        self.model.apply(add_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def profile_model(model, input_size, cuda):\n",
    "    \"\"\" Compute FLOPS and #Params of the CNN.\n",
    "\n",
    "    Arguments:\n",
    "        model (nn.Module): model which should be profiled.\n",
    "        input_size (tuple): size of the input variable.\n",
    "        cuda (bool): if True then variable will be upload to the GPU.\n",
    "\n",
    "    Returns:\n",
    "        dict:\n",
    "            dict[\"flops\"] (float): number of GFLOPs.\n",
    "            dict[\"params\"] (int): number of million parameters.\n",
    "    \"\"\"\n",
    "    profiler = ModelProfiler(model)\n",
    "    var = torch.zeros(input_size)\n",
    "    if cuda:\n",
    "        var = var.cuda()\n",
    "    profiler(var)\n",
    "    return {\"flops\": profiler.get_flops('G'), \"params\": profiler.get_params('M')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def GFLOPs_and_Parms_count_summary(block, input_tensor, block_name):\n",
    "    \n",
    "    flops, params = profile_model(block, input_tensor.size(), False).values()\n",
    "    \n",
    "    print('{0}\\n{1}\\n{0}'.format('-'*50, block_name))\n",
    "\n",
    "    print('GFLOPs:\\t\\t\\t\\t{}\\nNo. of params (in million):\\t{}'.format(flops, params))\n",
    "    \n",
    "    return block(input_tensor)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Running the below cell, you should get the following outputs:**\n",
    "\n",
    "```\n",
    "--------------------------------------------------\n",
    "init\n",
    "--------------------------------------------------\n",
    "GFLOPs:\t\t\t\t        3.9518208\n",
    "No. of params (in million):\t0.03872\n",
    "--------------------------------------------------\n",
    "maxpool\n",
    "--------------------------------------------------\n",
    "GFLOPs:\t\t\t\t        0.0\n",
    "No. of params (in million):\t0.0\n",
    "--------------------------------------------------\n",
    "layer1\n",
    "--------------------------------------------------\n",
    "GFLOPs:\t\t\t\t        5.6623104\n",
    "No. of params (in million):\t0.22144\n",
    "--------------------------------------------------\n",
    "layer2\n",
    "--------------------------------------------------\n",
    "GFLOPs:\t\t\t\t        9.437184\n",
    "No. of params (in million):\t1.475328\n",
    "--------------------------------------------------\n",
    "layer3\n",
    "--------------------------------------------------\n",
    "GFLOPs:\t\t\t\t        9.437184\n",
    "No. of params (in million):\t5.899776\n",
    "--------------------------------------------------\n",
    "layer4\n",
    "--------------------------------------------------\n",
    "GFLOPs:\t\t\t\t        2.8311552\n",
    "No. of params (in million):\t7.079424\n",
    "--------------------------------------------------\n",
    "up4\n",
    "--------------------------------------------------\n",
    "GFLOPs:\t\t\t\t        0.065536\n",
    "No. of params (in million):\t0.265216\n",
    "--------------------------------------------------\n",
    "up3\n",
    "--------------------------------------------------\n",
    "GFLOPs:\t\t\t\t        0.1572864\n",
    "No. of params (in million):\t0.199168\n",
    "--------------------------------------------------\n",
    "up2\n",
    "--------------------------------------------------\n",
    "GFLOPs:\t\t\t\t        0.1572864\n",
    "No. of params (in million):\t0.050432\n",
    "--------------------------------------------------\n",
    "up1\n",
    "--------------------------------------------------\n",
    "GFLOPs:\t\t\t\t        0.1572864\n",
    "No. of params (in million):\t0.012928\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "init\n",
      "--------------------------------------------------\n",
      "GFLOPs:\t\t\t\t3.9518208\n",
      "No. of params (in million):\t0.03872\n",
      "--------------------------------------------------\n",
      "maxpool\n",
      "--------------------------------------------------\n",
      "GFLOPs:\t\t\t\t0.0\n",
      "No. of params (in million):\t0.0\n",
      "--------------------------------------------------\n",
      "layer1\n",
      "--------------------------------------------------\n",
      "GFLOPs:\t\t\t\t5.6623104\n",
      "No. of params (in million):\t0.22144\n",
      "--------------------------------------------------\n",
      "layer2\n",
      "--------------------------------------------------\n",
      "GFLOPs:\t\t\t\t9.437184\n",
      "No. of params (in million):\t1.475328\n",
      "--------------------------------------------------\n",
      "layer3\n",
      "--------------------------------------------------\n",
      "GFLOPs:\t\t\t\t9.437184\n",
      "No. of params (in million):\t5.899776\n",
      "--------------------------------------------------\n",
      "layer4\n",
      "--------------------------------------------------\n",
      "GFLOPs:\t\t\t\t2.8311552\n",
      "No. of params (in million):\t7.079424\n",
      "--------------------------------------------------\n",
      "up4\n",
      "--------------------------------------------------\n",
      "GFLOPs:\t\t\t\t0.065536\n",
      "No. of params (in million):\t0.265216\n",
      "--------------------------------------------------\n",
      "up3\n",
      "--------------------------------------------------\n",
      "GFLOPs:\t\t\t\t0.1572864\n",
      "No. of params (in million):\t0.199168\n",
      "--------------------------------------------------\n",
      "up2\n",
      "--------------------------------------------------\n",
      "GFLOPs:\t\t\t\t0.1572864\n",
      "No. of params (in million):\t0.050432\n",
      "--------------------------------------------------\n",
      "up1\n",
      "--------------------------------------------------\n",
      "GFLOPs:\t\t\t\t0.1572864\n",
      "No. of params (in million):\t0.012928\n"
     ]
    }
   ],
   "source": [
    "# input data for model check\n",
    "input_tensor = torch.zeros(1, 3, 320, 320)\n",
    "\n",
    "# LinkNet architecture\n",
    "model = LinkNet(num_classes=5, encoder=\"vgg16\")\n",
    "\n",
    "input_tensor = GFLOPs_and_Parms_count_summary(model.init, input_tensor, 'init')\n",
    "input_tensor = GFLOPs_and_Parms_count_summary(model.maxpool, input_tensor, 'maxpool')\n",
    "input_tensor = GFLOPs_and_Parms_count_summary(model.layer1, input_tensor, 'layer1')\n",
    "input_tensor = GFLOPs_and_Parms_count_summary(model.layer2, input_tensor, 'layer2')\n",
    "input_tensor = GFLOPs_and_Parms_count_summary(model.layer3, input_tensor, 'layer3')\n",
    "input_tensor = GFLOPs_and_Parms_count_summary(model.layer4, input_tensor, 'layer4')\n",
    "input_tensor = GFLOPs_and_Parms_count_summary(model.up4, input_tensor, 'up4')\n",
    "input_tensor = GFLOPs_and_Parms_count_summary(model.up3, input_tensor, 'up3')\n",
    "input_tensor = GFLOPs_and_Parms_count_summary(model.up2, input_tensor, 'up2')\n",
    "input_tensor = GFLOPs_and_Parms_count_summary(model.up1, input_tensor, 'up1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Encoder-Decoder Implementation",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## <font style=\"color:green\">4.2. Check classifier & forward Implementation</font>\n",
    "\n",
    "**Input width and height is the same as output width and height because semantic segmentation predicts the label of each pixel.**\n",
    "\n",
    "**Running the below cell, you should get the following outputs:**\n",
    "```\n",
    "Prediction Size: torch.Size([1, 5, 320, 320])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Size: torch.Size([1, 5, 320, 320])\n"
     ]
    }
   ],
   "source": [
    "# input data for model check\n",
    "input_tensor = torch.zeros(1, 3, 320, 320)\n",
    "\n",
    "# LinkNet architecture\n",
    "model = LinkNet(num_classes=5, encoder=\"vgg16\")\n",
    "\n",
    "# examining the prediction size\n",
    "pred = model(input_tensor)\n",
    "print('Prediction Size: {}'.format(pred.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Classifier & Forward Implementation",
     "locked": true,
     "points": "15",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
