{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:blue\">Assignment: Implement the Focal Loss</font>\n",
    "\n",
    "We are already familiar with detection loss function; this is a combination of location loss and classification loss. Remember that we have used smooth L1 loss for location loss and OHEM loss for classification loss. \n",
    "\n",
    "In this assignment, you have to implement the **focal loss** for classification loss. Location loss will remain as it is.\n",
    "\n",
    "## <font color='blue'>Marking Scheme</font>\n",
    "\n",
    "#### Maximum Points: 30\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Problem</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>2. Focal loss Implementation</h3></td> <td><h3>30</h3></td> </tr>\n",
    "    </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:green\">1. Focal Loss</font>\n",
    "\n",
    "**Following is the screenshot form the RetinaNet lecture. It has the definition of focal loss.**\n",
    "\n",
    "---\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w9-focal_loss.png'>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "Originally this is published in the paper [Focal Loss for Dense Object Detection](https://arxiv.org/pdf/1708.02002.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:green\">2. Focal loss Implementation [30 Points]</font>\n",
    "\n",
    "We have defined the DetectionLoss class, where smooth L1 loss is already implemented. You do not have to make any changes in this part. **You have to implement the focal loss part**. \n",
    "\n",
    "Keep in mind that class targets with label `-1` must be ignored at the time of calculating the **focal loss**. The value of gamma we have chosen `2`; do not change it. \n",
    "\n",
    "**Hints:** \n",
    "\n",
    "- The following link may be useful to understand a few loss function implementation in PyTorch. Understanding those will be very helpful in the focal loss implementation.\n",
    "\n",
    "\n",
    "- https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss\n",
    "\n",
    "\n",
    "- https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy\n",
    "\n",
    "\n",
    "- https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.log_softmax\n",
    "\n",
    "\n",
    "- https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.nll_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Write your code where it is specified. Do not modify / delete other codes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DetectionLoss(nn.Module):\n",
    "    def __init__(self, num_classes, gamma=2, ignore_index=-1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # gamma will be uses in focal loss\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # ignore_index will be used classification loss.\n",
    "        # at the time of encoding, anchor boxes which 0.4 <IoU < 0.5, assign -1 as label.\n",
    "        # at the time of finding the classification loss these labels should be ignored\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, loc_preds, loc_targets, cls_preds, cls_targets):\n",
    "        '''Compute loss between (loc_preds, loc_targets) and (cls_preds, cls_targets).\n",
    "\n",
    "        Args:\n",
    "          loc_preds: (tensor) predicted locations, sized [batch_size, #anchors, 4].\n",
    "          loc_targets: (tensor) encoded target locations, sized [batch_size, #anchors, 4].\n",
    "          cls_preds: (tensor) predicted class confidences, sized [batch_size, #anchors, #classes].\n",
    "          cls_targets: (tensor) encoded target labels, sized [batch_size, #anchors].\n",
    "\n",
    "        loss:\n",
    "          (tensor) loss = (SmoothL1Loss(loc_preds, loc_targets),  FocalLoss(cls_preds, cls_targets)).\n",
    "        '''\n",
    "\n",
    "        ################################################################\n",
    "        # loc_loss\n",
    "        ################################################################\n",
    "        \n",
    "\n",
    "        pos = cls_targets > 0  # [N,#anchors]\n",
    "        num_pos = pos.long().sum(1, keepdim=True)\n",
    "\n",
    "        mask = pos.unsqueeze(2).expand_as(loc_preds)  # [N,#anchors,4]\n",
    "        masked_loc_preds = loc_preds[mask].view(-1, 4)  # [#pos,4]\n",
    "        masked_loc_targets = loc_targets[mask].view(-1, 4)  # [#pos,4]\n",
    "        loc_loss = F.smooth_l1_loss(masked_loc_preds, masked_loc_targets, reduction='none')\n",
    "        loc_loss = loc_loss.sum() / num_pos.sum().float()\n",
    "\n",
    "        ################################################################\n",
    "        # cls_loss with Focal Loss\n",
    "        ################################################################\n",
    "        \n",
    "        cls_loss = None\n",
    "\n",
    "        ###\n",
    "        cls_targets = cls_targets.view(-1)\n",
    "        cls_preds = cls_preds.view(-1, self.num_classes)\n",
    "        ce_loss = F.cross_entropy(cls_preds, cls_targets, reduction='none', \n",
    "                                  ignore_index = self.ignore_index)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        cls_loss = (1 * (1-pt)**gamma * ce_loss)\n",
    "        cls_loss = torch.mean(cls_loss[cls_targets != -1])        \n",
    "        ###\n",
    "        \n",
    "        return loc_loss, cls_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# <font style=\"color:green\">3. Check the implementation</font>\n",
    "\n",
    "**Before submitting the notebook, make sure you have verified your implementation.**\n",
    "\n",
    "Let's write a data encoder class to generate location labels and class labels. We are already familiar with this class in [Create Custom Single-stage Detector](https://courses.opencv.org/courses/course-v1:OpenCV+OpenCV-106+2019_T1/courseware/2ae52496773c42ba8216cca380ad4fd3/2c916b45595d459c8c7b944038512ba9/4?activate_block_id=block-v1%3AOpenCV%2BOpenCV-106%2B2019_T1%2Btype%40vertical%2Bblock%40400083ecaadf4cc392bfd643d899fd5c) section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class DataEncoder:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "        self.anchor_areas = [8 * 8, 16 * 16., 32 * 32., 64 * 64., 128 * 128]  # p3 -> p7\n",
    "        self.aspect_ratios = [0.5, 1, 2]\n",
    "        self.scales = [1, pow(2, 1 / 3.), pow(2, 2 / 3.)]\n",
    "        num_fms = len(self.anchor_areas)\n",
    "        fm_sizes = [math.ceil(self.input_size[0] / pow(2., i + 3)) for i in range(num_fms)]\n",
    "        self.anchor_boxes = []\n",
    "        for i, fm_size in enumerate(fm_sizes):\n",
    "            anchors = self.generate_anchors(self.anchor_areas[i], self.aspect_ratios, self.scales)\n",
    "            anchor_grid = self.generate_anchor_grid(input_size, fm_size, anchors)\n",
    "            self.anchor_boxes.append(anchor_grid)\n",
    "        self.anchor_boxes = torch.cat(self.anchor_boxes, 0)\n",
    "\n",
    "    def encode(self, boxes, classes):\n",
    "        iou = self.compute_iou(boxes, self.anchor_boxes)\n",
    "        iou, ids = iou.max(1)\n",
    "        loc_targets = self.encode_boxes(boxes[ids], self.anchor_boxes)\n",
    "        cls_targets = classes[ids]\n",
    "        cls_targets[iou < 0.5] = -1\n",
    "        cls_targets[iou < 0.4] = 0\n",
    "\n",
    "        return loc_targets, cls_targets\n",
    "    \n",
    "    def get_num_anchors(self):\n",
    "        return len(self.aspect_ratios) * len(self.scales)\n",
    "    \n",
    "    @staticmethod\n",
    "    def encode_boxes(boxes, anchors):\n",
    "        anchors_wh = anchors[:, 2:] - anchors[:, :2] + 1\n",
    "        anchors_ctr = anchors[:, :2] + 0.5 * anchors_wh\n",
    "        boxes_wh = boxes[:, 2:] - boxes[:, :2] + 1\n",
    "        boxes_ctr = boxes[:, :2] + 0.5 * boxes_wh\n",
    "        return torch.cat([(boxes_ctr - anchors_ctr) / anchors_wh, torch.log(boxes_wh / anchors_wh)], 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_anchor_grid(input_size, fm_size, anchors):\n",
    "        grid_size = input_size[0] / fm_size\n",
    "        x, y = torch.meshgrid(torch.arange(0, fm_size) * grid_size, torch.arange(0, fm_size) * grid_size)\n",
    "        anchors = anchors.view(-1, 1, 1, 4)\n",
    "        xyxy = torch.stack([x, y, x, y], 2).float()\n",
    "        boxes = (xyxy + anchors).permute(2, 1, 0, 3).contiguous().view(-1, 4)\n",
    "        boxes[:, 0::2] = boxes[:, 0::2].clamp(0, input_size[0])\n",
    "        boxes[:, 1::2] = boxes[:, 1::2].clamp(0, input_size[1])\n",
    "        return boxes\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_anchors(anchor_area, aspect_ratios, scales):\n",
    "        anchors = []\n",
    "        for scale in scales:\n",
    "            for ratio in aspect_ratios:\n",
    "                h = round(math.sqrt(anchor_area) / ratio)\n",
    "                w = round(ratio * h)\n",
    "                x1 = (math.sqrt(anchor_area) - scale * w) * 0.5\n",
    "                y1 = (math.sqrt(anchor_area) - scale * h) * 0.5\n",
    "                x2 = (math.sqrt(anchor_area) + scale * w) * 0.5\n",
    "                y2 = (math.sqrt(anchor_area) + scale * h) * 0.5\n",
    "                anchors.append([x1, y1, x2, y2])\n",
    "        return torch.Tensor(anchors)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_iou(src, dst):\n",
    "        p1 = torch.max(dst[:, None, :2], src[:, :2])\n",
    "        p2 = torch.min(dst[:, None, 2:], src[:, 2:])\n",
    "        inter = torch.prod((p2 - p1 + 1).clamp(0), 2)\n",
    "        src_area = torch.prod(src[:, 2:] - src[:, :2] + 1, 1)\n",
    "        dst_area = torch.prod(dst[:, 2:] - dst[:, :2] + 1, 1)\n",
    "        iou = inter / (dst_area[:, None] + src_area - inter)\n",
    "        return iou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Running the below cell, you should get the following outputs:**\n",
    "\n",
    "```\n",
    "Bounding Box Loss:\t  0.76351\n",
    "Classification Loss:\t1.0741\n",
    "```\n",
    "\n",
    "**Bounding box loss must match as its codes are all ready given.**\n",
    "\n",
    "**If the classification loss does not match but close to it; then there are the possibilities of the following:**\n",
    "\n",
    "\n",
    "- **If the loss is greater than expected loss:** You might not have taken care of the ignore index.\n",
    "\n",
    "\n",
    "- **If the loss is less than expected loss:** You might have taken care of the ignore index at the time of calculating the loss but might be forgotten at the time of taking the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding Box Loss:\t0.76351\n",
      "Classification Loss:\t1.0741\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_height = img_width = 300\n",
    "\n",
    "num_classes = 5  # including background\n",
    "\n",
    "bounding_boxes = torch.tensor([[100, 100, 150, 150], [120, 200, 160, 250]], dtype=torch.float)\n",
    "targets = torch.tensor([2, 4])\n",
    "\n",
    "data_encoder = DataEncoder((img_height, img_width))\n",
    "\n",
    "num_anchors = data_encoder.get_num_anchors()\n",
    "\n",
    "bboxes, labels = data_encoder.encode(bounding_boxes, targets)\n",
    "\n",
    "torch.manual_seed(21)\n",
    "\n",
    "pred_bboxes = torch.rand((1, labels.size()[0], 4))\n",
    "pred = torch.rand((1, labels.size()[0], num_classes))\n",
    "\n",
    "gamma = 2\n",
    "detection_loss = DetectionLoss(num_classes, gamma)\n",
    "\n",
    "bb_loss, cls_pred_loss = detection_loss(pred_bboxes, bboxes.unsqueeze(0), pred, labels.unsqueeze(0))\n",
    "\n",
    "print('Bounding Box Loss:\\t{0:.5}'.format(bb_loss))\n",
    "print('Classification Loss:\\t{0:.5}'.format(cls_pred_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Focal loss Implementation",
     "locked": true,
     "points": "30",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
